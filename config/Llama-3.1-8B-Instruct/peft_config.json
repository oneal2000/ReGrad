{
    "peft_type": "LORA",
    "r": 4,
    "lora_alpha": 8,
    "target_modules": [
        "model.layers.16.mlp.gate_proj",
        "model.layers.16.mlp.up_proj",
        "model.layers.16.mlp.down_proj",
        "model.layers.17.mlp.gate_proj",
        "model.layers.17.mlp.up_proj",
        "model.layers.17.mlp.down_proj",
        "model.layers.18.mlp.gate_proj",
        "model.layers.18.mlp.up_proj",
        "model.layers.18.mlp.down_proj",
        "model.layers.19.mlp.gate_proj",
        "model.layers.19.mlp.up_proj",
        "model.layers.19.mlp.down_proj",
        "model.layers.20.mlp.gate_proj",
        "model.layers.20.mlp.up_proj",
        "model.layers.20.mlp.down_proj",
        "model.layers.21.mlp.gate_proj",
        "model.layers.21.mlp.up_proj",
        "model.layers.21.mlp.down_proj",
        "model.layers.22.mlp.gate_proj",
        "model.layers.22.mlp.up_proj",
        "model.layers.22.mlp.down_proj",
        "model.layers.23.mlp.gate_proj",
        "model.layers.23.mlp.up_proj",
        "model.layers.23.mlp.down_proj",
        "model.layers.24.mlp.gate_proj",
        "model.layers.24.mlp.up_proj",
        "model.layers.24.mlp.down_proj",
        "model.layers.25.mlp.gate_proj",
        "model.layers.25.mlp.up_proj",
        "model.layers.25.mlp.down_proj",
        "model.layers.26.mlp.gate_proj",
        "model.layers.26.mlp.up_proj",
        "model.layers.26.mlp.down_proj",
        "model.layers.27.mlp.gate_proj",
        "model.layers.27.mlp.up_proj",
        "model.layers.27.mlp.down_proj",
        "model.layers.28.mlp.gate_proj",
        "model.layers.28.mlp.up_proj",
        "model.layers.28.mlp.down_proj",
        "model.layers.29.mlp.gate_proj",
        "model.layers.29.mlp.up_proj",
        "model.layers.29.mlp.down_proj",
        "model.layers.30.mlp.gate_proj",
        "model.layers.30.mlp.up_proj",
        "model.layers.30.mlp.down_proj",
        "model.layers.31.mlp.gate_proj",
        "model.layers.31.mlp.up_proj",
        "model.layers.31.mlp.down_proj"
    ]
}