{
    "peft_type": "LORA",
    "r": 8,
    "lora_alpha": 16,
    "target_modules": [
        "model.layers.14.mlp.gate_proj",
        "model.layers.14.mlp.up_proj",
        "model.layers.14.mlp.down_proj",
        "model.layers.15.mlp.gate_proj",
        "model.layers.15.mlp.up_proj",
        "model.layers.15.mlp.down_proj",
        "model.layers.16.mlp.gate_proj",
        "model.layers.16.mlp.up_proj",
        "model.layers.16.mlp.down_proj",
        "model.layers.17.mlp.gate_proj",
        "model.layers.17.mlp.up_proj",
        "model.layers.17.mlp.down_proj",
        "model.layers.18.mlp.gate_proj",
        "model.layers.18.mlp.up_proj",
        "model.layers.18.mlp.down_proj",
        "model.layers.19.mlp.gate_proj",
        "model.layers.19.mlp.up_proj",
        "model.layers.19.mlp.down_proj",
        "model.layers.20.mlp.gate_proj",
        "model.layers.20.mlp.up_proj",
        "model.layers.20.mlp.down_proj",
        "model.layers.21.mlp.gate_proj",
        "model.layers.21.mlp.up_proj",
        "model.layers.21.mlp.down_proj",
        "model.layers.22.mlp.gate_proj",
        "model.layers.22.mlp.up_proj",
        "model.layers.22.mlp.down_proj",
        "model.layers.23.mlp.gate_proj",
        "model.layers.23.mlp.up_proj",
        "model.layers.23.mlp.down_proj",
        "model.layers.24.mlp.gate_proj",
        "model.layers.24.mlp.up_proj",
        "model.layers.24.mlp.down_proj",
        "model.layers.25.mlp.gate_proj",
        "model.layers.25.mlp.up_proj",
        "model.layers.25.mlp.down_proj",
        "model.layers.26.mlp.gate_proj",
        "model.layers.26.mlp.up_proj",
        "model.layers.26.mlp.down_proj",
        "model.layers.27.mlp.gate_proj",
        "model.layers.27.mlp.up_proj",
        "model.layers.27.mlp.down_proj"
    ]
}